\documentclass[12pt]{article}

\title{\textbf{Generative Adversarial Networks for Text Generation}}
\author{Sven Vaupel}
\date{xx.xx.2017}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{mathrsfs}

% TODO: Split into different files
% TODO: Plugin anpassen!
% Searchterms for qick formula:
% find: >>(.+?)<<
% replace: \( $1 \)
\begin{document}

\maketitle
\setlength{\parindent}{0cm}

\section{Overview}

Giving a basic view over what is planned. Especially what I plan to describe before I tackle GANs!

\begin{enumerate}
  \item Notation ("copy" from [2])

  \item Neural Networks \\
    What are Neural Networks and how do they work. A brief introduction.

  \begin{enumerate}

    \item Use cases \\
      Giving a quick overview of Regression and Classification using simple examples.

    \item Important terms \\
      Giving a quick overview over the terminology used specifically in machine learning

      \begin{enumerate}

        \item Hyperparameter \\
          The Hyperparameters' values are set prior to the actual training process.

        \item Stochastic gradient

        \item Over- / Underfitting

      \end{enumerate}

    \item Important concepts \\
      Concepts which are important for the explanation of later topics

    \begin{enumerate}

      \item Backpropagation \\
        Why is it Important and what are the requirements.

      \item Minibatch Training

      \item Dropout

      \item Latent space? \\
        Could be of interest for some of the papers.
        What Information can be drawn from it. Showing some examples.

    \end{enumerate}

  \end{enumerate}

  \item GAN
\end{enumerate}


\section{Generative Adversarial Networks}

Appart from classification and data prognosis there is the field of data generation.
The different networks existing for this task mostly consist of nondifferentiable functions, thus the function's gradient has to be estimated to use backpropagation during the network's training process.
This estimation is one of the main causes for errors in such networks.
\{Could add information on some of these Networks? See book chapter 20\}
\\
\\
Goodfellow et al \cite{1} presented a different approch to tackle the generation of data according to a set of examples.
The method called generative adversarial networks (GAN) connects the output of a differentiable data generating network to the input of a drscriminative multilayer perceptron.
Connecting the networks' in- and outputs in such fashion, it is now possible to train the generator by minimizing the probability that the attached discriminator is able to correctly distinguish between example- and artificially generated data.


  \subsection{Model Definition}

  The foundation for this training is a game theoretic scenario where the generator \(  G  \) tries to beat the adversary \(  D  \) in a zero sum game.
  Based on the set of data \(  x  \), by defining a noise prior \(  p_z(z)  \) it is possible to provide \(  G(z; \theta_g)  \) which represents a mapping from the noise variable \(  z  \) to the given data space \( x \) where \( \theta_d \) are the parameters of a multiplayer perceptron representing the differentiable function \( G \).
  As obvious \( G \) serves as the generating network which creates data space samples \( x \) from a random noise \( z \).
  The discriminator on the other hand takes any samples \( x \) from the data space and assigns them a scalar value according to the probability whether the sample originates from the original training data or the data generated by \( G \).
  Similar to \( G \)'s definition, we construct a differentiable function \( D(x; \theta_d) \) with underlying perceptron parameters \( \theta_d \) representing the discriminator.
  \( D \) then calculates the odds for \( x \) originating from the training distribution \( p_d \) rather than the generated distribution \( p_g \).
  \\
  \\
  Regarding the goal of learning the generator's distribution \( p_g \) over the data space, the GAN's learning process can be described as a competition of \( G \) and \( D \) in a two player minimax game using the value function \( V(D,G) \) so that the descriminators payoff is calculated by:

  \[
  min_g max_d V(G,D) =  \mathbb{E}_{x \sim p_d}[\log D(x; \theta_d)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z; \theta_g); \theta_d))]
  \]

  \cite[Equation (1)]{1}

  \subsection{Training Process}

  Training is then done by maximizing the probablility of \( D \) assigning the correct label to the samples coming from either the training distribution \( p_d \) or the generated distribution \( p_g \) while simultaniously minimizing the probability of \( G \) generating samples to which \( D \) confidently assigns the training data as origin.
  In other words we train the discriminator to be able to sort out artificial samples while at the same time training the generator to fool the discriminator with its immitations.
  This process results in an equilibrium where even a maximally trained discriminator can not distinguish generated from original samples thus assigning \( D(x) = 1/2 \) to every input.
  When this happens we reached the point where the generated and the trainning distribution match such that \( p_g = p_d \).

  Implementation wise it is only possible to approach the training procedure with an iterative numerical process.
  This approach is shown in Algorithm 1:

    \pagebreak
    % \subsubsection{Algorithm 1:}
    \textbf{Algorithm 1:}
    \input{gan_alorithm_1}
    \cite[Algorithm (1)]{1}
  \\
  \\
  As stated in algorithm 1 the inner loop does not train the discriminator to completion but limits the learning process by applying a constraining hyperparameter \( k \).
  A completely trained perceptron \( D \) would result in overfitting on the training distribution \( p_d \) focusing the generator on creating exaxt copies of the training examples.
  Therefore the algorithm keeps \( D \) near its optimum by alternating between training \( D \) for \( k \) steps and training G for one step.
  Regarding the network's early training phase, \( \log ( 1 - D(G(z)) ) \) could saturate in cases where \( D \) confidently assigns correct labels to all data samples.
  Thus maximizing \( \log D(G(z)) \) provides significantly higher gradients than minimizing \( \log( 1 - D(G(z)) ) \) without harming the algorithm's dynamics.

  \section{Theoretical Results and Boundries}



% examples
% \emph{emph}\footnote{footnote}
% $\rightarrow$

\bibliography{bibliography.bib}{}
\bibliographystyle{plain}
\end{document}

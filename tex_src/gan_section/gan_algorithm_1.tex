% TODO: copy
% TODO: Make more self explanatory
% TODO: Make pretty!

\caption{ Training process of generative adversarial networks.}
 \label{gan_algorithm_1}
\begin{itemize}
  \item Applying minibatch training for data sampling.
  \item Using stochastic gradient descent to optimize the discriminator and generator.
  \item Applying hyperparameter \( k \). Resulting in a relation  of \( k:1 \) training steps between the discriminator and the generator.
\end{itemize}
\hrulefill

\begin{description} \item
for \textit{ number of training iterations } do

  \begin{description}
  \item for \textit{ k steps } do

    \begin{enumerate}

    \item Sample minibatch of \( m \) noise samples \( \{z_1, ... ,z_m \} \) from noise prior \( p_g(z) \)
    \item Sample minibatch of \( m \) noise samples \( \{x_1, ... ,x_m \} \) from data generation distribution \( p_d(x) \)
    \item Update the discriminator by ascending its stochastic gradient:
    \[
      \nabla_{\theta_d} \sum_{i=1}^m \big[\log D(x_i) + \log ( 1 - D(G(z_i)) ) \big]
    \]

    \end{enumerate}

  \item end for
  \end{description}

  \begin{enumerate}

    \item Sample minibatch of \( m \) noise samples \( \{z_1, ... ,z_m \} \) from noise prior \( p_g(z) \)
    \item Update the generator by descending its stochastic gradient:
    \[
      \nabla_{\theta_g} \sum_{i=1}^m \big[\log ( 1 - D(G(z_i)) ) \big]
    \]

  \end{enumerate}

\item end for
\end{description}
